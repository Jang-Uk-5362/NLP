{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jang-Uk-5362/data-science/blob/main/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8TJkXkpDnSq"
      },
      "source": [
        "!pip install ratsnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpjMKSVWs6B2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99853b6a-ad12-4208-86d0-8329c4c2fa26"
      },
      "source": [
        "#  Linking Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jsq7yrjWYTB"
      },
      "source": [
        "## GPT 입력값 만들기\n",
        "\n",
        "GPT 모델 입력값을 만들려면 Byte-level Byte Pair Encoding 어휘집합 구축 결과(`vocab.json`, `merges.txt`)가 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/wordpiece`)에 있어야 합니다. 다음을 수행해 이미 만들어 놓은 BBPE 어휘집합을 포함한 GPT 토크나이저를 `tokenizer_gpt`라는 변수로 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocab Tutorial 먼저 수행"
      ],
      "metadata": {
        "id": "oLm5gy6xm5Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJRVVpC-WyIc"
      },
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer_gpt = GPT2Tokenizer.from_pretrained(\"/gdrive/My Drive/nlpbook/bbpe\")\n",
        "tokenizer_gpt.pad_token = \"[PAD]\" # Add a special token"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n3JlkUbZobu"
      },
      "source": [
        "예시 문장 세 개를 각각 토큰화해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp48wVBIZtYj"
      },
      "source": [
        "# Tokenize 3 sentences\n",
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_gpt.tokenize(sentence) for sentence in sentences]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ahPBg-Zx71"
      },
      "source": [
        "토큰화 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__DWz4djZz_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea7d71a-5129-46d7-88c9-2c0861a6ac27"
      },
      "source": [
        "# Check tokenization results\n",
        "tokenized_sentences"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ìķĦ', 'ĠëįĶë¹Ļ', '..', 'Ġì§Ħì§ľ', 'Ġì§ľì¦ĿëĤĺ', 'ëĦ¤ìļĶ', 'Ġëª©ìĨĮë¦¬'],\n",
              " ['íĿł',\n",
              "  '...',\n",
              "  'íı¬ìĬ¤íĦ°',\n",
              "  'ë³´ê³ł',\n",
              "  'Ġì´ĪëĶ©',\n",
              "  'ìĺģíĻĶ',\n",
              "  'ì¤Ħ',\n",
              "  '....',\n",
              "  'ìĺ¤ë²Ħ',\n",
              "  'ìĹ°ê¸°',\n",
              "  'ì¡°ì°¨',\n",
              "  'Ġê°Ģë³į',\n",
              "  'ì§Ģ',\n",
              "  'ĠìķĬ',\n",
              "  'êµ¬ëĤĺ'],\n",
              " ['ë³Ħë£¨', 'Ġìĺ', 'Ģëĭ¤', '..']]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFQWvXsFXC1H"
      },
      "source": [
        "이번 배치의 크기가 3이라고 가정하고 이번 배치의 입력값을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATwWfngCXQXK"
      },
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "batch_inputs = tokenizer_gpt(\n",
        "    sentences,\n",
        "    padding=\"max_length\",\n",
        "    max_length=12,\n",
        "    truncation=True,\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9g0tJ9uXj6O"
      },
      "source": [
        "`batch_inputs`의 내용을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4pnC6DjXkv6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df606904-9954-4150-a848-5ba74493da48"
      },
      "source": [
        "batch_inputs.keys()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJBRdJegXsZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e30483-13de-4bcc-e3e4-975a05d929bd"
      },
      "source": [
        "batch_inputs['input_ids']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[334, 2338, 263, 581, 4055, 464, 3808, 0, 0, 0, 0, 0],\n",
              " [3693, 336, 2876, 758, 2883, 356, 806, 422, 9875, 875, 2960, 7292],\n",
              " [4957, 451, 3653, 263, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_XT5QTsXvBG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902d4db9-0b33-4424-b7f4-09689d1c1602"
      },
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzWd3u-vXz9c"
      },
      "source": [
        "## BERT 입력값 만들기\n",
        "\n",
        "BERT 모델 입력값을 만들려면 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/wordpiece`)에 워드피스 어휘집합 구축 결과(`vocab.txt`)가 있어야 합니다. 다음을 수행해 위에서 이미 만들어 놓은 워드피스 어휘집합을 포함한 BERT 토크나이저를 `tokenizer_bert`라는 변수로 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTFeKKzJX-O7"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer_bert = BertTokenizer.from_pretrained(\n",
        "    \"/gdrive/My Drive/nlpbook/wordpiece\",\n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pcxOXJxaER5"
      },
      "source": [
        "예시 문장 세 개를 각각 토큰화해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9AzWXcvaJKH"
      },
      "source": [
        "# Tokenize 3 sentences\n",
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "tokenized_sentences = [tokenizer_bert.tokenize(sentence) for sentence in sentences]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJhOHq-maNLw"
      },
      "source": [
        "토큰화 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdzBTLFPaPFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f850c08-aaaf-4b00-fcb0-dd5076635651"
      },
      "source": [
        "tokenized_sentences"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['아', '더빙', '.', '.', '진짜', '짜증나', '##네요', '목소리'],\n",
              " ['흠',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '포스터',\n",
              "  '##보고',\n",
              "  '초딩',\n",
              "  '##영화',\n",
              "  '##줄',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '.',\n",
              "  '오버',\n",
              "  '##연기',\n",
              "  '##조차',\n",
              "  '가볍',\n",
              "  '##지',\n",
              "  '않',\n",
              "  '##구나'],\n",
              " ['별루', '였다', '.', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMxTUoowYHb2"
      },
      "source": [
        "이번 배치의 크기가 3이라고 가정하고 이번 배치의 입력값을 만들어 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_sFosQjYIE3"
      },
      "source": [
        "sentences = [\n",
        "    \"아 더빙.. 진짜 짜증나네요 목소리\",\n",
        "    \"흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\",\n",
        "    \"별루 였다..\",\n",
        "]\n",
        "batch_inputs = tokenizer_bert(\n",
        "    sentences,\n",
        "    padding=\"max_length\",\n",
        "    max_length=12,\n",
        "    truncation=True,\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohNW5R3zYOW5"
      },
      "source": [
        "`batch_inputs`의 내용을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZiMSoaKYTUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633a3c9d-3cde-4c9f-e86b-3f1b1e61412d"
      },
      "source": [
        "batch_inputs.keys()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pWaGuSdYZGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e2bb35c-f6e9-4a3d-ffc7-b7933a0904be"
      },
      "source": [
        "batch_inputs['input_ids']"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 620, 2631, 16, 16, 1993, 3678, 1990, 3323, 3, 0, 0],\n",
              " [2, 997, 16, 16, 16, 2609, 2045, 2796, 1981, 1346, 16, 3],\n",
              " [2, 3274, 9510, 16, 16, 3, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djtA4xkIYbOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16807e5e-b86f-49f9-d0c1-6b25c6b19325"
      },
      "source": [
        "batch_inputs['attention_mask']"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVR15VXZYoTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fcf446d-8b20-4ccd-b300-86da201cb58c"
      },
      "source": [
        "batch_inputs['token_type_ids']"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w3bMoev0oTHf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}